{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of doc2vec for edX MOOCs dataset\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the first part of the analysis applied to the corpus of edX MOOCs.\n",
    "\n",
    "We trained doc2vec model on the corpus collected using https://github.com/TokyoTechX/web-crawler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Loading the corpus\n",
    "\n",
    "We specify the directory where the textual data of 285 courses is stored. \n",
    "\n",
    "Each MOOC contains textual data from its html, video transcript and assessment components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "foldername = \"/home/zarina/Documents/OEDO/web-crawler/HTMLs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The data was preprocessed so that all the punctuation, numerical symbols and stop words were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,re,json\n",
    "import codecs,string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize,wordpunct_tokenize\n",
    "from collections import Counter \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def course_list(foldername,comp_type_code):\n",
    "    \n",
    "    courses = os.listdir(foldername)     \n",
    "    selected_comp_list = comp_type_selection(comp_type_code)\n",
    "    print(selected_comp_list)\n",
    "    course_path = []\n",
    "    for idx,course in enumerate(courses):\n",
    "        course_path.append(os.path.join(foldername,course))\n",
    "\n",
    "    return selected_comp_list,course_path\n",
    "\n",
    "def comp_type_selection(code):\n",
    "\n",
    "    selected_comp = []\n",
    "    if code[0] == '1':\n",
    "        selected_comp.append('all_textcomp.json')\n",
    "    if code[1] == '1':    \n",
    "        selected_comp.append('all_videocomp.json')\n",
    "    if code[2] == '1':    \n",
    "        selected_comp.append('all_probcomp.json')\n",
    "        \n",
    "    return selected_comp\n",
    "\n",
    "def text_preprocessing(raw,code):\n",
    "    \n",
    "    list_of_words = []\n",
    "    raw = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\",'', raw)\n",
    "    token = wordpunct_tokenize(raw)\n",
    "    stopword_obj = stopwords.words('english')\n",
    "    list_of_words=[]\n",
    "    steming_type = PorterStemmer()\n",
    "    for i in token:\n",
    "        processed_tmp = i.lower()\n",
    "        \n",
    "        if len(processed_tmp) == 1:\n",
    "            continue\n",
    "        \n",
    "        if code[0] == '1':\n",
    "            if not processed_tmp.isalpha():\n",
    "                continue\n",
    "        else:\n",
    "            if not processed_tmp.isalnum():\n",
    "                continue\n",
    "\n",
    "        if code[1] == '1':\n",
    "            if processed_tmp in stopword_obj:\n",
    "                continue\n",
    "     \n",
    "        if code[2] == '1':\n",
    "            processed_tmp = steming_type.stem(processed_tmp)\n",
    "        \n",
    "        list_of_words.append(processed_tmp)\n",
    "\n",
    "    return list_of_words\n",
    "    \n",
    "def extract_text_from_component(foldername,comp_type_code,processing_code):\n",
    "    \n",
    "    selected_comp_types, all_course_paths = course_list(foldername,comp_type_code)\n",
    "    clean_text_set =  dict()\n",
    "    raw_text_set = dict()\n",
    "    course_name_set = []\n",
    "    for course_path in all_course_paths:\n",
    "        course_name = os.path.basename(course_path)\n",
    "        all_text = []\n",
    "        all_raw_text = []\n",
    "        for comp_type in selected_comp_types:\n",
    "            with open(os.path.join(course_path,comp_type),'r',encoding='utf-8') as file:\n",
    "                if comp_type == 'all_videocomp.json':\n",
    "                    dict_parser = json.loads(file.read())\n",
    "                    for main_key, main_value in dict_parser.items():\n",
    "                        if type(main_value['transcript_en']) is dict:  \n",
    "                            continue\n",
    "                        raw_txt = main_value['transcript_en']\n",
    "                        clean_text = text_preprocessing(raw_txt,processing_code)\n",
    "                        if not clean_text:\n",
    "                            continue\n",
    "                        all_text += clean_text\n",
    "                else:\n",
    "                    dict_parser = json.loads(file.read())\n",
    "                    for main_key, main_value in dict_parser.items():\n",
    "                        raw_txt = main_value['content']\n",
    "                        all_raw_text.append(raw_txt)\n",
    "                        clean_text = text_preprocessing(raw_txt,processing_code)\n",
    "                        if not clean_text:\n",
    "                            continue\n",
    "                        all_text += clean_text\n",
    "\n",
    "        clean_text_set[course_name] = all_text\n",
    "        raw_text_set[course_name] = all_raw_text\n",
    "        course_name_set.append(course_name)\n",
    "        \n",
    "    return course_name_set,clean_text_set,raw_text_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all_textcomp.json', 'all_videocomp.json']\n"
     ]
    }
   ],
   "source": [
    "comp_type_code = '110'  # 1st digit = TEXTCOMP (1),  2ND digit = VIDEOCOMP(1), 3RD digit = PROBCOMP(1):  ex. 100 = only textcomp active \n",
    "processing_code = '110' # 1st digit = alpha(1),alphanumeric(0),  2ND digit = filter stopword(1) , 3RD digit = do stemming(1)\n",
    "\n",
    "course_name_set,clean_text_set,raw_text_set=extract_text_from_component(foldername,comp_type_code,processing_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the course labels (categories) from csv file. The labels are used for the classification task later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colnames = ['course', 'subject']\n",
    "df = pd.read_csv('test.csv')\n",
    "csv_map=dict(zip(list(df.course), list(df.subject)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load text data into the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global_Social_Change\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "all_courses = []\n",
    "course_to_cat={}\n",
    "course_cnt = 0\n",
    "categories = set()\n",
    "\n",
    "for cur_name in course_name_set:\n",
    "    cur_name_csv=cur_name.replace('-_', '-').replace('_', ' ') # remove discrepancy in course labels\n",
    "    if cur_name_csv in csv_map:\n",
    "        if course_cnt==0:\n",
    "            print (cur_name)\n",
    "        cat=csv_map[cur_name_csv]\n",
    "        categories.add(cat)\n",
    "        doc_i=TaggedDocument(clean_text_set[cur_name], [cur_name])\n",
    "        all_courses.append(doc_i)\n",
    "        course_cnt+=1\n",
    "        course_to_cat[cur_name]=cat\n",
    "    \n",
    "#input to doc2vec\n",
    "doc_list = all_courses[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting up doc2vec model\n",
    "\n",
    "We use two models:\n",
    "* doc2vec based on DBOW (dm=0)\n",
    "* doc2vec based on DM (dm=1)\n",
    "    \n",
    "with the document vector length of 200, and context window size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc19,s0.001,t8)\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t8)\n",
      "CPU times: user 25min 34s, sys: 1.04 s, total: 25min 35s\n",
      "Wall time: 3min 20s\n",
      "CPU times: user 4min 58s, sys: 488 ms, total: 4min 59s\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "models = [\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, min_count=19, epochs=20, workers=cores),\n",
    "    # PV-DM w/average\n",
    "    Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, min_count=19, epochs =20, workers=cores),\n",
    "]\n",
    "\n",
    "models[0].build_vocab(doc_list)\n",
    "print(str(models[0]))\n",
    "models[1].reset_from(models[0])\n",
    "print(str(models[1]))\n",
    "for model in models:\n",
    "    %%time model.train(doc_list, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating similarity between MOOCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow+w,d200,n5,w8,mc19,s0.001,t8) \n",
      "\n",
      "Most similar courses to  Compliance_in_Office_365-_Data_Governance  are: \n",
      " [('Compliance_in_Office_365-_eDiscovery', 0.6079649925231934), ('Microsoft_SharePoint_2016-_Search_and_Content_Management', 0.4997135102748871), ('Office_365-_SharePoint_Online_Administrator', 0.496964693069458), ('Provisioning_Office_365_Services', 0.469723641872406), ('Microsoft_SharePoint_2016-_Authentication_and_Security', 0.46946612000465393), ('Microsoft_SharePoint_2016-_Workload_Optimization', 0.46710318326950073), ('Microsoft_SharePoint_2016-_Infrastructure', 0.43522870540618896), ('Microsoft_SharePoint_Online_for_Site_Administrators', 0.4127456545829773), ('Configuring_SharePoint_Hybrid', 0.393161416053772), ('Microsoft_SharePoint_2016-_Productivity_Solutions', 0.39244675636291504)] \n",
      "\n",
      "Doc2Vec(dm/m,d200,n5,w8,mc19,s0.001,t8) \n",
      "\n",
      "Most similar courses to  Compliance_in_Office_365-_Data_Governance  are: \n",
      " [('Compliance_in_Office_365-_eDiscovery', 0.6992765665054321), ('Office_365-_SharePoint_Online_Administrator', 0.6061806678771973), ('Microsoft_SharePoint_2016-_Authentication_and_Security', 0.6029965877532959), ('Microsoft_SharePoint_2016-_Search_and_Content_Management', 0.5976791381835938), ('Microsoft_SharePoint_2016-_Workload_Optimization', 0.5593832731246948), ('Microsoft_SharePoint_2016-_Productivity_Solutions', 0.5336045622825623), ('Provisioning_Office_365_Services', 0.5288414359092712), ('Microsoft_SharePoint_2016-_Infrastructure', 0.525094211101532), ('Configuring_SharePoint_Hybrid', 0.511488676071167), ('Microsoft_SharePoint_Online_for_Site_Administrators', 0.4774423837661743)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checks\n",
    "for model in models:\n",
    "    print (model, '\\n')\n",
    "    sims = model.docvecs.most_similar(course_name_set[1])\n",
    "    print (\"Most similar courses to \", course_name_set[1], \" are: \\n\",  sims, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare similarities between words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.20589416  0.49491093 -0.11974067 -0.30048046]\n",
      "[ 0.33363882  0.55245304  0.13124876 -0.10792442]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54820123721292613"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (model0.wv['java'][0:4])\n",
    "print (model0.wv['python'][0:4])\n",
    "model0.wv.similarity('java', 'python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the trained models for fast inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    models[i].save(\"model\" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded model can be retrained later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Doc2Vec.load(\"model0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on gensim doc2vec https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
